## Clustering

### 1. Revisiting the Inverted Indices for Billion-Scale Approximate Nearesr Neighbors:

最早能处理十亿级数据的索引结构是基于倒排索引（IVF）。它先用 K-Means 将特征空间划成 Voronoi 区域，每个区域对应一个聚类中心，检索时只需在少量相关区域中找候选，能在几十毫秒内获得不错的召回率。

后来提出了倒排多重索引（IMI），将向量空间切分成多个正交子空间，并分别对每个子空间做 K-Means 分区。最终的空间划分是各子空间区域的笛卡尔积，因此区域数量巨大，非常细粒度，每个区域里数据很少，使得候选集更精确，内存和速度都更高效。

但 IMI 的区域结构也带来问题：区域数量虽然理论上巨大，但大部分区域其实没有数据（空区域）。因为各子空间独立做 K-Means，但真实数据（尤其是 CNN 特征）的子向量之间并非独立，存在较强相关性。结果导致搜索时会浪费时间访问大量空区域，降低检索效率和最终性能。



#### 1.1 相关技术：

==PQ：==

一种有损压缩技术，用于把高维向量压缩成很小的编码（通常是几十个字节），常用于大规模向量数据库放不进内存的情况。

做法概念很简单：
把一个 D 维向量切成 M 个子向量，每个子向量做独立的 K-Means 量化。
每个子空间有一个码本（codebook），码本通常有 256 个 codewords（这样一个 codeword 用 1 字节即可表示）。

**codebook（码本）**就是一个“代表向量的集合”，用来近似替代原来的子向量。
通俗理解：
把一个子空间里的许多相似子向量，用少量“典型样本”来代表，这些典型样本就组成 codebook。更正式地说：
 • PQ 会把 D 维向量切成 M 个子向量，每个子空间做一次 K-Means；
 • K-Means 得到的聚类中心（例如 256 个）就是这个子空间的 **codebook**；
 • 每个 codeword 是一个聚类中心，用来表示某个子向量的近似值。

因此，一个向量最终被编码成 M 个字节（每个字节是某个码本中 codeword 的索引），而不必存储原始向量，从而达到压缩效果。

压缩后向量不需要解压就能近似计算与查询向量的欧氏距离。计算时用预先计算好的查找表（lookup table），只需 M 次查表与相加即可完成距离估算，这就是所谓的 ADC（Asymmetric Distance Computation）。



==IVFADC:==

IVFADC 是一种大规模向量检索方法，它通过两点来提高速度和节省内存：
 ① 用倒排索引（IVF）减少候选数量；
 ② 用产品量化（PQ）压缩向量。

具体流程：

• 先用 K-Means 将向量空间划成 K 个区域，每个区域是一个 Voronoi Cell，对应一个中心（codebook $C = \{c_1 ... c_K\}$）。
• 每个数据库向量只会被分配到离它最近的中心所在的区域，因此搜索时只需要查少数几个区域，而不是全局搜索。
• IVFADC 不直接量化原始向量，而是量化“向量相对区域中心的偏移量”（residual / displacement）。
• 偏移量使用 PQ 编码，而且使用的是全局共享的 PQ codebooks（所有区域共用同一套 PQ 码本）。



==IMI:==

IMI 是对传统倒排索引（IVF）的升级。传统 IVF 用一个全维 K-Means 码本来划分空间，而 IMI 将向量空间切分成多个子空间（一般是 2 个），并分别为每个子空间训练一个独立的 K-Means 码本。

做法要点：
 • 将 D 维向量分成两个 D/2 维子向量；
 • 为每个子空间训练一个 K-Means 码本（各有 K 个聚类中心）；
 • 最终的空间划分不是单独使用某一个码本，而是这两个子码本区域的 **笛卡尔积**；

假设向量被分成两半：
 • 前半部分用码本 A 聚成 K=3 类：A1、A2、A3
 • 后半部分用码本 B 聚成 K=3 类：B1、B2、B3

如果像 IVF 一样只用一个码本，你只得到 3 个区域。
 但 IMI 会组合生成：
 (A1,B1)、(A1,B2)、(A1,B3)、(A2,B1)…一直到 (A3,B3)

总区域数 = 3 × 3 = 9

这就是笛卡尔积：把两个子空间的聚类结果“两两组合”。

 → 因此区域数量从 K 增长为 K²，非常巨大。

好处：
 区域变得极度细粒度，每个区域包含的数据更少，搜索时只需访问极小部分区域就能找到最近邻，提高速度。

IMI 还提出了 “multi-sequence” 算法，用于高效找出与查询向量最接近的区域顺序。

在压缩方式上，IMI 仍使用 PQ 对向量的残差（相对区域中心的偏移量）进行编码，并共享 PQ 码本。



#### 1.2 Inverted Index Revisited

==与IMI的对比：==

![image-20251111191926280](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251111191926280.png)

K代表codebook sizes(centroids的数量)

IMI 的划分非常细，但这种“极细粒度”带来了问题：

1）**IMI 搜索时需要访问更多区域**
 因为区域太多、每个区域包含的数据太少，为凑出足够候选（candidates），IMI 必须跳来跳去访问很多区域。
 每跳一次就是一次随机内存访问，而随机访问比 PQ 查表慢，尤其当 PQ 码长很短时，这会拖慢搜索速度。

2）**IMI 的性能随着 K 增大更快饱和，不如 IVF 受益大**
 扩大码本大小 K 能让区域划分更精细。
 但实验发现：
 • 对 IMI，K 增大后划分质量提升有限，很快“饱和”；
 • 对 IVF，提高 K 效果更明显，可以持续提升聚类精度。

![image-20251111192549356](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251111192549356.png)

3）**IMI 的内存开销随 K² 增长，非常快变大**
 IMI 要维护 K² 个倒排链表（因为两个子空间码本笛卡尔积）。
 如 K = 2¹⁵，则区域数 = 2³⁰，对十亿数据需要多额外 ~4 字节/向量，这很可观。
 因此 IMI 对大码本 **不节省内存**，甚至变得昂贵。

得出结论：

• 用更大码本的倒排索引（IVF）比 IMI 更好：
 虽然 IVF 的区域数量比 IMI 少，但增大 K 后，IVF 的分区质量提升更明显。

• 但 **阻碍 IVF 使用超大码本的唯一问题**是：
 查询时需要把 query 分配到 K 个中心中最近的几个，而这个过程是 O(K) 的，K 大时会变得很慢。

• 作者指出，现在这个问题已经可以解决：
 由于近几年 ANN（近似最近邻搜索）技术进步，可以用“高精度的近似搜索”来完成 query 的聚类中心分配，不再需要精确比对所有 K 个中心。



==实现：==

**分组（grouping）和子区域剪枝（pruning）**，用来提高压缩精度和检索效率。核心解释如下：

1.  **目的**
     在 IVFADC 中，每个倒排索引区域（region）包含许多向量，直接用 PQ 压缩整个区域的残差会有一定误差。为提升压缩精度和召回率，作者提出把每个区域再划分成更小的 **子区域（subregion）**，类似 Voronoi 区域。
2.  **分组（grouping）方法**

-   每个区域都有一个中心 $c$，找到它的 L 个邻居中心 $s_1, ..., s_L$。
-   每个区域的子中心单独训练，以其中一个区域的中心 $c$ 为例，有 ${x1...xn}$ 的点属于这个区域，现在要把这些点重新分配给各个子中心 
-   子中心（subcentroid）不是用 K-Means 训练，而是用 **凸组合**：

$$
\text{subcentroid} = c + \alpha (s_l - c)
$$

其中 $\alpha$ 是可学习的缩放因子，保证内存消耗很小，不需要存储完整的子码本。每个邻居中心都会产生一个新的子中心。

-   原本以 $c$ 为中心的点被分配到最近的子中心，同一个子中心的向量连续存储，形成组。（比如 $l=4$ ,这样原本都属于中心为 $c$ 区域的点就又被分成了4个子区域）

**压缩优化**

-   向量相对于子中心的残差比相对于 $c$ 的残差更小，因此 PQ 可以更精确压缩（同样码长下误差更小）。
-   提升了压缩精度和召回率。

**距离计算（query 时）**

![image-20251111200143709](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251111200143709.png)

-   计算查询向量到压缩向量的距离，可以通过拆分公式，把距离分成几部分：

     1）query 到区域中心的距离
     2）query 到子中心的距离（可重用，避免重复计算）
     3）PQ 残差部分

-   还有一个与 query 无关的常数项，量化后存储在每个点的编码中。

3.   **子区域剪枝（pruning）**

-   在搜索时，不必访问每个子区域，只访问最有可能包含最近邻的子区域。
-   实践中可以过滤掉一半子区域而不降低精度，从而加速搜索。

4.   **缩放因子学习**

-   $\alpha$ 通过最小化向量到子中心距离学习得到（约束在 [0,1] 内，使子中心在区域中心和邻居中心之间）。
-   先固定每个向量的子中心索引，求最佳 $\alpha$；再更新 $\alpha$ 得到闭式解。

5.   **效果**

-   分组+剪枝提高了候选列表质量和压缩精度，从而提升整体检索性能。
-   这套方法在倒排索引（IVF）上非常有效，但在 IMI 上难以发挥，因为 IMI 的区域数量太多，分组和剪枝的额外开销过大。

==实验:==

$Recall@R$ measure which is calculated as a rate of queries for which the true nearest neigh bor is presented in the short-list of length R

![image-20251112101210226](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251112101210226.png)

对于 **SIFT1B 数据集**（SIFT 特征是直方图形式），IMI 在小候选数（R 较小）时略优，因为：
 → SIFT 向量可自然分成几乎独立的两部分（图像不同区域的特征），非常适合 IMI 这种“把高维空间拆成两个子空间分别聚类”的做法。

但当 **R > 2¹³** 时，两者性能相当。

不过，IMI 的 **运行开销更大**：
 → 它需要多次随机内存访问（访问多个小区域），
 → 并使用效率较低的 **multi-sequence algorithm** 来遍历候选区域，
 → 导致搜索时间明显更慢。

**搜索时间：**

![image-20251112100645386](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251112100645386.png)

横着画线，对应相同的R@1/10，然后比较 x 轴的 $t$ 大小 

![image-20251112100753374](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251112100753374.png)



### 2. Learning to Route in Similarity Graphs:

**问题：局部最优困境（local minima）**

-   在实际应用中，贪心路由可能会**陷入局部最优**：查询可能停留在某个子最优节点，而无法到达真正的最近邻。
-   这种情况会导致搜索结果不准确。

![image-20251113164505951](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251113164505951.png)

**论文的提出方法：学习路由函数**

-   作者提出通过**学习（learn）路由函数**来解决局部最优问题。
-   核心思路是**引入全局结构信息**，不仅仅依赖局部最近邻信息。
-   具体做法是：对图的每个节点增加一个可学习的表示（representation），使得从起始节点到查询最近邻的路由能更加“智能”，避免卡在局部最优节点。

 **Learning to Search（L2S）** 方法及其特点，可以分解如下理解：

1.  **方法本质**：L2S 是一类用于 **结构化预测（structured prediction）** 的方法，它通过学习如何在可能解的空间中“搜索”来找到最优解。换句话说，它不直接预测结果，而是学习 **搜索策略**。
2.  **核心机制**：
    -   引入一个 **参数化模型（parametric model）** 来描述搜索过程。
    -   通过训练调整这些参数，使模型尽量模仿或逼近 **最优搜索策略（optimal search strategy）**。
    -   在训练中，模型像一个“搜索智能体（search agent）”，学习跟随 **专家策略（expert search procedure）**。
    -   专家策略通常由能产生最优结果的算法提供，例如在机器翻译任务中生成最优翻译。

#### 2.1 可学习的随机搜索

在相似性图中进行最近邻搜索的经典方法（beam search）及其改进为可学习的随机搜索

1.  **传统 Beam Search（算法 1）**：

    -   从一个初始顶点 $v_0$ 开始，维护两组数据：已访问顶点集 $V$ 和候选顶点堆 $H$，堆中存储每个顶点到查询 $q$ 的距离。
    -   每次从堆中选择**距离查询最近的顶点** $v_i$ 进行扩展，把其邻居加入堆（若未访问过）。
    -   当堆为空或达到运行预算（如距离计算次数上限）时停止，最后返回距离查询最近的 k 个顶点。

2.  **随机化（Stochastic Search）**：

    -   不再总是选择最小距离的顶点，而是**根据 softmax 概率( 所有点相加概率为1）采样**下一个顶点（随机选择）：
        $$
        P(v_i|q,H) = \frac{e^{-d(v_i,q)/\tau}}{\sum_{v_j \in H} e^{-d(v_j,q)/\tau}}
        $$

    -   采样结束后，从已访问顶点集 $V$ 中再次根据 softmax 采样 k 个顶点作为搜索结果。

    对每个访问过的顶点 $v_i \in V$，计算它与查询 $q$ 的相似度分数（如 $f(v_i)\cdot g(q)$）。

    用 softmax 把这些分数转成概率：越相似的点，概率越大。

    根据这个概率分布，从 $V$ 里采样 $k$ 个点作为输出结果。

    -   当 $\tau$ → 0⁺ 时，softmax 的分布会变得非常“尖锐”——概率几乎全集中在分数最高（距离最近）的顶点上。此时算法就**退化为普通的贪心 beam search**，每次都选最优顶点。

        当 $\tau$ > 0 时，softmax 分布更平滑，算法在选择下一个顶点时会**有一定概率选到次优点**，引入随机性，从而能探索更多路径、避免陷入局部最优。

3.  **引入可学习映射**：

    -   将距离 $d(v_i, q)$ 替换为顶点和查询的可学习向量内积：
        $$
        P(v_i|q,H) = \frac{e^{<f(v_i),g(q)>}}{\sum_{v_j \in H} e^{<f(v_j),g(q)>}}
        $$

    -   $f(v_i)$ 是数据库点的神经网络映射，$g(q)$ 是查询query的神经网络映射。

        原本 beam search 里使用欧氏距离 $d(v_i, q)$ 来判断哪个顶点更接近查询；

        现在改为用两个神经网络 $f(v_i)$ 和 $g(q)$ 的**内积（dot product）** 作为相似度：

        内积 $f(v_i) \cdot g(q)$ 越大，表示两个向量越相似；内积大 ⇒ 向量方向接近 ⇒ 特征相似 ⇒ 数据点更相似

        而搜索算法通常需要**“距离越小越相似”**的度量；

        所以用负号把内积变成“伪距离”：
        $$
        d(v_i, q) = - f(v_i) \cdot g(q)
        $$
        这样越相似（内积大）的点，负内积就越小，被认为“距离更近”。

        这让模型能**通过学习映射函数 f 和 g 来优化搜索路径**。

4.  **最终输出步骤**：

    -   由于训练中使用的是内积而非原始欧氏距离，需要对 top-k 检索结果**根据原始欧氏距离重新排序**，保证返回真正的最近邻。

#### 2.2 最佳路由：

定义一个理想的“**参考函数**” $\text{Ref}(H)$：（这个函数每次都可以通过图上跳数找到离真实最近邻最近的顶点）

-   这个函数能从当前候选集合 $H$ 中选出**离真实最近邻 $v^\*$** 最近的顶点，
-   “近”的衡量不是欧氏距离，而是**图上跳数（hops）**——即通过图边需要几步能到达 $v^*$。

==**最优路由序列（optimal routing sequence）**：==

-   如果一个搜索算法在每次迭代时，都选择 $v_i = \text{Ref}(H)$，
     也就是**总是扩展那个离真实最近邻最近的节点**；
-   并且最终找到了真实最近邻 $v^*$，
-   那么这条顶点访问序列就称为“最优路由”。

最后的要求：

-   当搜索结束后，算法输出的 top-k 结果中应包含 $\text{Ref}(V)$，
     即那个真正的最近邻节点。

==**如何在训练阶段实际构造 Ref(H) 的值**：==

1.  **计算 Ref(H)**

    -   对每个训练查询 $q$ 和候选顶点集合 $H$，用 **广度优先搜索（BFS）** 计算每个顶点到真实最近邻 $v^*$ 的图跳数（hop count）。

2.  **加速训练的做法**

    -   在训练开始前 **预计算所有训练查询的跳数**，避免每次迭代重复 BFS。(训练特指神经网络训练)

    模型是神经网络，需要优化

    -   f(v) 和 g(q) 是可学习的神经网络映射，它们一开始的参数是随机初始化的，
    -   一次计算或一次前向传播无法让模型学会正确的“选择最优顶点”的策略。

    -   这些预计算结果存储在 **持久化缓存** 中，训练时按需读取。

3.  **节省内存的策略**

    -   不存储所有顶点到 $v^*$ 的跳数，只存储“可能会被搜索访问的顶点”。
    -   用启发式方法筛选这些顶点：
        -   如果存在一条**从起点到 $v^\*$ 的近似最优路径经过它**，那么该顶点被选中，
        -   具体来说，考虑所有路径长度最多比最优路径多 5 跳的顶点。

==**训练可学习搜索模型以执行最优路由的目标函数（loss function）：**==：

1.  **训练目标**

    -   模型的目标是**尽量让随机搜索模仿最优路由（Opt(q)）**，也就是让每一步选择的顶点概率最大化。

    -   最直观的方法（naive approach）是**最大化最优路径的对数似然**：
        $$
        J_{\text{naive}} = \mathbb{E}_{q,v} \Big[ \log P(\text{Opt}(q)|q) \Big]
        $$

        -   其中 $\text{Opt}(q)$ 表示查询 q 的最优路由序列（Ref(H) 指定的顶点序列），
        -   P(vi|Hi) 是在每一步随机搜索中模型选择该顶点的概率，
        -   P(v | TopK, q, V) 是最终 top-k 输出中包含真实最近邻 v 的概率。

    Eq,v[⋅] 表示“对所有训练查询 $q$ 和对应的真实最近邻 $v$ 的平均值”。

    -   $q$ 是训练查询
    -   $v$ 是查询 $q$ 的真实最近邻点

    **作用**

    -   训练目标 $J_{\text{naive}}$ 需要对所有训练样本进行平均，使模型学习到一般性的搜索策略，而不是只针对某一个查询。
    -   在实现中，这通常就是**对训练数据 batch 求平均**，类似标准神经网络的 loss 计算方式。

2.  **展开形式**
    $$
    J_{\text{naive}} = \mathbb{E}_{q,v} \Big[ \sum_{v_i \in H_i, H_i \in \text{Opt}(q)} \log P(v_i | q, H_i) + \log P(v \in \text{TopK} | q, V) \Big]
    $$

    -   第一项：每一步沿最优路径选择的顶点概率最大化
    -   第二项：确保最终 top-k 输出中包含真实最近邻 v

3.  **意义**

    -   训练时，通过最大化这个目标函数，模型学会在图中选择“最优顶点”，并最终把最近邻包含在 top-k 结果里。
    -   当 top-k 中包含真实最近邻时，即使搜索是随机的，通过 rerank 也能保证成功找到最近邻。

    **问题所在：**

    -   如果训练只让模型模仿最优路径（optimal routing trajectory），模型只见过“理想状态”的候选集合 $H$；
    -   没有学习如何处理**偏离最优路径的情况**。

    **应用到新查询时的风险**

    -   一旦模型在某一步选择了非最优顶点（犯错），这个错误顶点会加入堆 $H$；
    -   下一步的候选集合就和训练时完全不同，模型没有见过这种状态。
    -   结果是：模型可能连续犯错，搜索性能大幅下降。

    **核心问题**

    -   模型缺乏**错误补偿能力（error recovery）**，只模仿理想专家，不能应对实际运行中可能出现的偏离。

    ==**改进训练策略：**==

    1.  **核心思想**

        -   允许模型按照当前参数 $f(), g()$ 在图中搜索，即便它可能选到次优顶点；
        -   搜索结束后，再用最优路由的监督信号去更新参数，让模型**在已经访问的顶点上尽量跟随最优路径**。
        -   这样模型学到的不只是理想状态，而是“如何从可能出现的错误状态中纠正回最优路径”。

        ![image-20251113161023283](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251113161023283.png)

    2.  **新的训练目标（J_imit）**
        $$
        J_{\text{imit}} = \mathbb{E}_{q,v} \Big[ \sum_{v_i \in H_i^{\text{Search}(q)}} \log P(v_i = \text{Ref}(H_i) | q, H_i) + \log P(v \in \text{TopK} | q, V) \Big]
        $$

        -   这里 $H_i^{\text{Search}(q)}$ 是**模型当前参数生成的搜索轨迹**，而不是仅仅理想轨迹；
        -   每个访问过的顶点都用 Ref(H) 作为目标进行监督，确保模型学习在错误状态下也能纠正。

    3.  **区别于原始目标（J_naive）**

        -   J_naive 只考虑最优轨迹，模型未见过错误状态；
        -   J_imit 考虑**模型实际可能走过的轨迹**，训练更稳健，对新查询性能更好。

==**计算训练目标 $J_{\text{imit}}$ 的梯度和 top-k 选择概率**：==

1.  **梯度计算**（要使得这个概率最大，要求导）
    -   第一项 $\log P(v_i = \text{Ref}(H_i) | q, H_i)$ 可以直接对模型参数 $\theta$ 求导，因为 $P(v_i|q,H_i)$ 是由公式 (2) 的 softmax 定义的，可微分。
    -   这一部分就是标准的监督信号，让模型学会在每一步访问的顶点上尽量跟随最优路由。
2.  **第二项：top-k 概率**
    -   这项是 **真实最近邻 v 被选入最终 top-k 输出的概率**。
    -   理论上，要计算 exact probability，需要考虑从已访问顶点 V 中选 k 个顶点的所有组合，这对大 k 是不可行的。
3.  **近似方法**
    -   采用类似 Wiseman & Rush 的采样方法：
        -   先从候选集合 V（不含 v）中**依概率采样 k-1 个顶点**，不放回；
        -   再计算 v 在剩余集合中被选中的概率。
    -   这样就得到一个**可微的近似 top-k 概率**，因为选顶点时用的是 softmax 概率，梯度可以通过概率传播回神经网络参数 f 和 g；

==**实验设置和评价指标**：==

1.  **计算预算限制（DCS）**
    -   为每次查询设置**最大距离计算次数（Distance Computations, DCS）**，例如 128、256、512。
    -   不同 DCS 对应低、中、高 Recall@1 的实验情境。
    -   R@x 的计算是在搜索返回的 **Top-k 候选结果** 中进行的，如果 `x ≤ k`，就统计真实最近邻是否出现在前 x 个中
2.  **评价指标：Recall@R**
    -   计算在 top-R 候选中包含真实最近邻的查询比例。
    -   是主要性能指标，用于衡量路由算法在有限计算预算下的准确性。
3.  **训练注意事项**
    -   **每个 DCS 值需要单独训练**，使得顶点表示 f(v) 能适应不同的计算预算限制。
4.  **实验图结构**
    -   使用 **Hierarchical Navigable Small World 图** 的底层作为搜索图，模型在此基础上学习可训练路由表示。

![image-20251113185707666](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251113185707666.png)

==总结：==

本文提出了一种**可学习的图路由算法**用于最近邻搜索（NNS）。它通过**学习节点表示（embedding）**，让搜索过程更容易沿着最优路径到达真正的最近邻。实验表明，在相同计算预算下，它比传统方法更不容易陷入局部最优、召回率更高。代价是需要**离线训练一个神经网络**，但训练完成后在线搜索阶段几乎不增加额外开销。



### 3. DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node

现有 ANNS 索引需要全部放在内存中，因此成本高且无法扩展到十亿规模。DiskANN 利用 SSD + 内存混合设计，让十亿点搜索在普通单机上也能做到高召回、低延迟和高数据密度。在经典 SIFT1B 数据上，它的准确率和速度都显著超过 FAISS、HNSW 等主流方法，并引入了新的图索引结构 Vamana，使整体性能更强。

==现有数据储存方案：==

在亿级或十亿级的大规模数据上，当前工业界主要有两类方案：

第一类为“倒排索引 + 压缩”（如 FAISS、IVFOADC+G+P），通过聚类将数据划分为 M 个簇，只搜索最接近的 m≪M 个簇；为了减少内存，这些方法使用 PQ 对向量进行有损压缩，虽然可在 <5ms 内检索，但由于压缩误差，1-recall@1 常仅约 0.5，只能在较弱的 1-recall@100 指标上取得高值。

第二类方法将数据分片，每片构建完整内存索引，如 NSG，但这需要极大的内存。例如 NSG 在 100M×128 维数据上需约 75GB 内存，因此 10 亿规模往往要用几十台服务器并行服务（如淘宝将 20 亿点划为 32 片部署）。

两类方法都依赖索引完全驻留内存，一旦索引放到 SSD 上，随机访问延迟会让搜索吞吐量急剧下降。FAISS 官方也指出“搜索必须在内存中进行，因为即使是 SSD 也慢几个数量级”。SSD 的随机读通常需要数百微秒，一块普通 SSD 每秒只能处理约 30 万次随机读，而在线搜索希望查询延迟控制在几毫秒以内。因此，要让 SSD 承载高性能 ANN 索引，必须：
 (a) 将每次查询的随机 SSD 访问次数压到几十次以内；
 (b) 将访问 SSD 的往返次数控制在 5–10 次以内。
 而若直接把 HNSW/NSG 等内存索引搬到 SSD 会导致每次查询数百次随机读，延迟不可接受。

==作者提出方案：==

DiskANN 利用新提出的 **Vamana 图索引** 实现了在 **64GB RAM** 上处理 **十亿级高维数据**，并达到 **95%+ 1-recall@1** 和 **<5ms** 延迟，证明消费级 SSD 也能支撑高性能 ANN 搜索。Vamana 生成的图直径更小，可显著减少 SSD 读取次数，并在内存中同样优于 HNSW/NSG。此外，大数据集可分块构建并高效合并，且系统通过“磁盘存图 + 全精度向量、内存存压缩向量”实现低成本与高精度兼得。

#### 3.1 Roubust Prune算法：

![image-20251114143855025](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251114143855025.png)

输入：点 $p$、候选邻居集合 $V$、出度上限 $R$ 以及距离阈值。

核心步骤：(对于一个顶点 $v \in V$，**从 v 出发的边**叫做 **出边（out-edge）**)

1.  初始化 $V$ 为候选集合并加上 $p$ 当前出边的邻居，清空 $p$ 的出边。（ \ {p} 表示从集合里面除去 p)
2.  在 $V$ 中选择距离 $p$ 最近的点 $p'$ 添加为出边。
3.  更新 $V$，删除所有距离比 $p'$ 更远的点。
4.  重复直到 $V$ 为空或出边达到上限 $R$。

设计思想：

SNG 保证搜索能找到最近邻，但图直径可能很大（如一维线性排列的点，直径 $O(n)$），导致磁盘存储时搜索需要大量顺序读。

Robust Pruning 则通过引入“距离缩放因子 $>1$”的约束：搜索路径上，每步距离查询点都缩短至少一个固定比例，从而使搜索**对数级收敛**。

#### 3.2 Vamana算法：

结合Greedy Search和Robust Prune：

![image-20251114145455639](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251114145455639.png)

Vamana 仅选择一个远小于 $n-1$ 的候选集合 $V$ 来执行 Robust Prune，从而加速索引构建，同时保持图的高性能搜索特性。

![image-20251114145035909](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251114145035909.png)

Vamana 索引算法通过迭代构建有向图 $G$ 来支持高效的 GreedySearch：

1.  **初始化图**：

    -   每个顶点随机选择 $R$ 个出邻居（out-neighbors）。

2.  **选择起点**：

    -   取数据集 P 的中位点（medoid） $s$ 作为 GreedySearch 的起点。

3.  **迭代优化图**：

    -   按随机顺序遍历每个点 $p \in P$。

    -   对当前图 $G$ 运行 `GreedySearch(s \to p)`，得到经过的点集合 $V_p$。

    -   使用 `RobustPrune(p, V_p, R)` 更新 $p$ 的出边，使其更适合收敛到 $p$。

    -   为每个 $p' \in N_\text{out}(p)$ 添加**反向边** $(p' \to p)$，确保搜索路径上的节点与 $p$ 相连。

    -   如果添加反向边导致某节点出度超过 $R$，再次运行 `RobustPrune` 限制出度。

    -   以一个点作为例子：

        ![image-20251114151552680](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251114151552680.png)

        先通过GreedySearch找到从中心点s出发到这个点坐标需要经过的所有节点集合为V

        然后

        ![image-20251114151729778](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251114151729778.png)

        通过RobustPrune更新这个点与这些邻居节点的连接（出边：从这个点出去连到这些邻居）

        然后

        ![image-20251114151908621](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251114151908621.png)

        对于被选择作为出边的点，还需要为它们添加反向边，使得搜索路径上的节点能互相连接

        同时在这里添加出边的时候，它们自己本身也会有出邻居，因此再添加反向边之后，出边个数可能会超出R，需要再次进行剪枝

        添加反向边时，**边的方向是从每个出邻居 $p' \in N_\text{out}(p)$ 指向 $p$**，即增加的是 $p'$ 的出度，而不是 $p$ 的出度。

        如果某个 $p'$ 已经有接近 R 的出度，新增 $p' \to p$ 的反向边就可能使 $p'$ 的出度超过 R。

4.  **多轮迭代**：

    -   算法总共做两轮遍历：

        -   第一轮使用 $\alpha=1$
        -   第二轮使用用户定义的 $\alpha_1$

        a 用来保证搜索路径每步距离查询点都有“足够”下降。

        如果节点 p 的候选邻居 q 满足
        $$
        d(q, \text{query}) \le \frac{d(p, \text{query})}{a}
        $$
        才保留 q 作为出邻居。

        换句话说，a 控制 **搜索路径的收敛速度**：

    -   第二轮可以生成更优的图，但如果第一轮也用较大 $\alpha_1$，会产生平均更高出度的图，导致构建速度变慢。

**总体效果**：随着迭代进行，图结构越来越适合 GreedySearch，搜索速度和准确率不断提升。

![image-20251114153336960](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251114153336960.png)

与 HNSW、NSG 的构建思路对比：

1.  **可调参数 α**：
    -   HNSW 和 NSG 没有可调 α，默认 α = 1。
    -   Vamana 可调 α，使得在图的 **出度（degree）** 与 **直径（diameter）** 间实现更好的权衡。
2.  **候选集合 V 的定义**：
    -   HNSW：V 是 GreedySearch 输出的最终 L 个候选点。
    -   Vamana / NSG：V 是 GreedySearch 遍历过程中访问过的所有节点。
    -   影响：Vamana / NSG 可以增加**长程边**，而 HNSW 只能加局部边，因此 HNSW 需要额外构建**多层图（hierarchy）**来弥补搜索范围。
3.  **初始图构建**：
    -   NSG：从近似 KNN 图开始，构建耗时且占内存。
    -   HNSW：从空图开始。
    -   Vamana：从**随机图**开始，实验证明比空图得到更高质量的图。
4.  **遍历次数**：
    -   HNSW / NSG：只遍历一次数据集。
    -   Vamana：遍历两次，第二轮进一步优化图质量，提高搜索性能。

#### 3.3 储存和取：

DiskANN 的目标是在单机 64GB 内存 + 普通 SSD 上完成十亿级向量的近似最近邻搜索。
主要困难有两个：

**（1）如何构建十亿点的图？内存放不下。**

直接跑 Vamana 不 可能，因为：

-   十亿点 × 100 维的原向量数据远超 64GB 内存；
-   Vamana 构图需要访问所有点向量。

在构建大规模 ANN 索引时，一个常见想法是：先用 k-means 把数据分成多个小簇（shards），每个簇分别构建自己的图索引。查询时，只把 query 发到和它最接近的几个簇中搜索。这样单个簇的索引可以放入内存，从而解决“10 亿点数据放不下”的问题。

但这种传统方式有两个明显缺点：

1.  **延迟变高**：查询必须被路由到多个簇，意味着多次索引访问与多次搜索启动。
2.  **吞吐量下降**：每个查询都要处理多个簇，因此整体系统压力增大，完成同样 QPS（Queries per second）需要更多计算。

DiskANN 的核心创新是反过来思考：
 与其在查询时把 query 发往多个簇，不如在**构建阶段**就让每个数据点属于多个簇，也就是说 —— **让簇之间重叠**。

**DiskANN 的方案：重叠聚类 + 合并多个小图**

步骤：

1.  用 k-means 将 10 亿数据分成 k 个簇，k≈40。
2.  每个点不只属于一个簇，而是属于 **r 个最近的簇中心**（通常 r=2）。
     这两个簇的交叠确保不同簇之间有足够连接。
3.  每个簇只有 N/k 点（例如 10 亿 / 40 = 2500 万），可以在内存内跑 Vamana。
4.  最终把所有簇的图简单做 **边集合并**（union），得到一个统一大图。

**为什么可行？**
 因为重叠使得图有足够跨簇的“桥”，GreedySearch 即使跨簇也能找到最短路径。

------

**（2）搜索时向量数据没法全部放进内存，怎么计算距离？**

DiskANN 的做法：

-   原始向量（float）存 SSD。
-   内存中只保存 **压缩后的向量**（例如 PQ 压缩后 32 bytes）。
-   搜索的距离计算使用压缩向量，虽然不精确但**足够好**。
-   但 Vamana 构图使用的是**全精度**向量，因此图结构非常精确，能很好地引导 GreedySearch。

这就避免了“向量放不进内存”的问题，同时保持高 recall。

**具体储存方式：**

DiskANN 将压缩向量存于内存，将全精度向量与图结构放在 SSD 上。为便于快速定位，磁盘中每个点采用定长记录：先存全精度向量，再存 R 个邻居 ID，不足部分以 0 填充。这样只需一次乘法即可计算任意点的磁盘偏移，避免在内存中维护额外指针，并使 SSD 能连续读取多个点记录。

由于 SSD 随机读取的延迟较高，若按传统 GreedySearch 方式逐点取邻居，会产生大量 round-trip，导致查询延迟不可接受。为此，DiskANN 采用 BeamSearch：在每轮迭代中利用内存中的 PQ 压缩向量快速选出候选集合 L 中距离最近的 W 个点，并一次性从 SSD 读取它们的邻居记录。因为读取多个相邻扇区几乎与读取一个扇区的成本相同，这种批量 I/O 能显著减少随机访问次数。所有新邻居再依据压缩距离加入 L 并重新筛选，只需少量迭代即可收敛。（W=1的时候就相当于GreedySearch)

W 的选择需要在“减少 I/O 往返”和“避免过多无效计算”之间取得平衡。虽然基于 NAND-flash 的 SSD 理论上每秒可以处理 50 万次以上的随机读取，但要达到这个最大吞吐量，必须让 SSD 的所有 I/O 队列长期处于“排队饱和”状态。这样虽然吞吐量高，但会带来显著的副作用：所有读请求都要在队列中等待，导致每次随机读取的延迟上升到毫秒级，明显拉高 ANN 查询延迟。

为了保持低延迟，DiskANN 不能把 SSD 压满运行，而需要让设备维持较低的负载因子（load factor）。实验显示，使用较小的 Beam Width（如 W = 2,4,8）可以让 SSD 的使用率稳定在约 30–40%，既不会造成 I/O 排队，也能维持较快的响应时间。在这种设置下，搜索线程的时间分布大致为：40–50% 用于 SSD I/O（因为仍需读取若干邻居记录），剩余时间用于压缩向量距离计算和候选更新。

DiskANN 为了减少查询过程中的 SSD 访问次数，引入了两个关键机制：**热点节点缓存（Caching）** 和 **基于全精度向量的隐式重排序（Implicit Re-ranking）**。

首先，在图搜索中，有些节点会被反复访问，例如距离起点 s（通常为 medoid）3~4 跳以内的节点，因为 Greedy/Beam 搜索在早期阶段总会经过这些区域。DiskANN 会将这些“频繁访问节点”的数据直接缓存到 DRAM 中，这样每次搜索都能在内存中完成前几跳的扩张，而不需要反复访问 SSD。

第二，DiskANN 解决 PQ（Product Quantization）带来的“距离误差”问题。PQ 是有损压缩，搜索时用 PQ 距离选出的候选点与真实欧氏距离并不完全一致，可能导致 Top-k 排序出现误差。DiskANN 的做法是：在 SSD 上将每个节点的“邻居列表 + 全精度向量”存放在同一个 4KB 扇区内。SSD 的随机读最小单位就是 4KB，因此读取节点邻居时顺带把完整向量也读入了内存，并不会增加额外的 I/O 成本。这样 BeamSearch 在扩展节点时就能逐步收集所有访问节点的全精度向量，最终使用真实距离重新排序 Top-k，确保结果精度接近内存型 ANN 系统。

==评估测试：==

![image-20251117165052579](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251117165052579.png)

对十亿级数据集的实验主要比较了两种 Vamana 构建方式：**One-Shot Vamana（单次构建）\**与\**Merged Vamana（分片合并）**。在 One-Shot 模式中，作者直接在 10⁹ 条 SIFT 数据上构建单一图索引，参数设为 L=125、R=128、α=2。这种方式需要极大的内存支持：在 M64-32ms（约 1.8 TB 内存）上运行约 2 天，峰值内存达到 1100 GB，最终得到一个平均度数约 1139 的高密度图。相比之下，Merged Vamana 通过“分片 + 合并”降低构建压力：首先用 k-means 将数据划分成 40 个子集，并将每个点分配给其最邻近的两个簇；随后在每个子集中用较小的 R=64 构建局部图，最后将所有子图的边集并入一个全局索引。这种方法只需 64GB 内存，整个过程在 z840 上约 5 天即可完成，最终索引规模为 348GB、平均度数为 921，大幅降低了构建资源需求。

从查询性能来看，单索引的结构更完整，搜索路径更短，因此**在相同 recall 下延迟更低**。例如，在 bigann 的 10k-query 测试中，One-Shot Vamana 以 <5ms 的延迟达到了接近 98.68% 的 1-recall@1，刷新了当时的单机 SSD-based ANN 搜索性能记录。而 Merged Vamana 因为每个点的边集在合并后受到 shard 限制（大约只覆盖全体数据的 5%），导致访问邻域需要跨更多边，搜索会多走若干跳，因此整体延迟比单索引高约 20%。尽管如此，Merged Vamana 依旧明显优于当时其他最先进方法，而且能够在**仅 64GB 内存下完成十亿级数据集索引构建**，在资源受限场景中是一种非常实用的方案。

整体来看，One-Shot Vamana 是追求极致性能的最佳选择，而 Merged Vamana 则在资源需求与性能之间取得了非常实际的工程折中。

==总结：==

作者提出了新的图索引算法 Vamana，在高召回场景下，其内存搜索性能达到或超过现有最优方法。同时，作者利用仅 64GB 内存，在十亿级数据上构建了基于 SSD 的 DiskANN 索引，并实现了毫秒级延迟。整体而言，他们把图方法的高召回/低延迟优势与压缩方法的高可扩展性结合起来，达成了当前规模下的最新性能水平。



### 4. Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search

==摘要：==

研究怎样把超大规模的近邻搜索任务切成多个更小的“分片”，并希望每个分片都能保留邻居关系，这样查询时只需要查少数几个分片。论文提出了新的“路由算法”，在查询时快速判断该查哪些分片，而且这些路由方法是“模块化”的——可以跟任何分区方式搭配，不像以前的方法强依赖特定的分区结构。

==问题及解决方案：==

作者指出：图索引虽然召回最高，但无法分布式扩展，因为图的遍历依赖性强、跨机器通信大。因此分布式系统普遍采用“先分片再路由”的方式：把数据分成多个 shard；查询时先用轻量级路由判断要查哪些 shard，再在这些 shard 内做本地搜索（图索引或暴力搜索）。这其实就是 IVF 的思想，分片质量决定性能。

接着作者指出：现有路由方法往往绑定特定的分片方式，例如 k-means 的 routing 就是“把 query 距离最近的中心挑出来”。但如果换成别的分区方法，往往没有对应的 routing。

然后引出最近被发现的“平衡图划分（Balanced Graph Partitioning）”：先构建 kNN 图，再对图做平衡分区，尽可能减少跨 shard 的边，这样最多邻居会落在同一个 shard 里。实验证明这种分片质量远高于 k-means，但有两个致命缺点：
 1）这种 shard 没有几何形状，无法像 k-means 那样靠“中心”做路由，只能训练代价很高的神经网络路由模型。
 2）必须先构建 kNN 图才能做图划分，而 kNN 图本身又需要 ANNS，形成“鸡生蛋”问题。



作者针对前面提到的平衡图划分问题提出了解决方案，让它能用于十亿级别的 ANNS。主要贡献有四点：

1.  **快速粗略建图**：用递归球切分（dense ball-carving）快速构建一个粗略的 kNN 图就够了，虽然近似，但能保证分片质量和查询性能。
2.  **快速、精确、模块化的组合式路由**：设计了两种通用路由方法，可搭配任何分区，尤其是图划分。
    -   **KRT**：先对每个 shard 做层次 k-means 子聚类，再用树或 HNSW 找最近中心点，把 query 路由到对应 shard。
    -   **HRT**：用 LSH，把 query 放在 LSH hash 排序里，找附近点对应的 shard。
3.  **路由的理论保证**：对 HRT 的两个变体进行了理论分析，证明每个 query 高概率会路由到包含近似最近邻的 shard，这是路由环节首次有严格理论保证。
4.  **实验验证**：在十亿级数据上，使用平衡图划分的 shards 比最优的竞品分区快 1.19–2.14×，top-shard 的邻居集中度高 25%，KRT 的训练时间也快几个数量级（半小时搞定十亿点），比神经网络路由方法快很多，同时 recall 更好或相当。

#### 4.1 快速近似 k-NN 图构建方法：

作者方法是递归划分 dense ball 簇：先随机选一些 pivots，把每个点分到最近的枢轴簇中，然后簇内递归继续划分或生成簇内两两边。直觉是 top-k 邻居大概率会落在同一簇。为了提升图质量，他们做了**独立重复**和**在第一次递归时把点分配给多个 pivots**，避免指数级开销。

关键点：

-   图不需要精确 top-k，只要捕捉粗略局部结构即可保证分片质量。
-   实验发现，即便图 recall 很低 (<0.3)，查询时 top-10 邻居仍有 96% 聚集在同一 shard：

-   增加每点边数（degree）有时会略微降低查询 recall，因为在低质量近似图中，多边会“污染”结构，但在精确图中不会出现。

结论：**构建稀疏、粗略的近似 k-NN 图就够用于分片，不会显著影响查询性能**。

####  4.2 **重叠分片减少边界点的召回损失**：

当分片是互斥的（disjoint）时，边界点的 k-NN 可能跨 shard，导致查询丢邻居。作者提出一种贪心算法（受 Fiduccia & Mattheyses 图划分启发）：复制节点，把它放到包含大多数邻居的 shard，从而减少跨 shard 边（cut edges），也就减少召回损失。

==**cut-edge 的来源**==

1.  你先有一张 **k-NN 图**：
    -   每个点连接它的 k 个最近邻
    -   这些连接构成图中的边
2.  然后你对所有点进行 **分片（sharding / partitioning）**：
     比如把数据分成 16 个 shard。
3.  **cut-edge 的定义**：
     如果一条 k-NN 边连接的两个点被分到了不同的 shard，这条边就叫 **cut-edge**。

也就是说：

-   **cut-edge = 跨分片的 k-NN 边**
-   **当一个点的邻居被划到别的 shard，查询时就可能 miss 掉它 → 召回下降**

关键细节：

-   引入参数 o ≥ 1 限制节点复制量，同时保持 shard 最大大小不变，通过增加 shard 数量实现。
-   算法每次选择能消除最多 cut edges 的节点进行复制，重复直到没有满足条件的节点。
-   可以批量并行处理：多个节点如果消除相同数量的 cut edges 可以同时处理。
-   只需少量轮次，因为每个节点至少消除一个 cut edge，k-NN 图的度数有限。

总结一句话：**重叠分片通过复制边界节点，把大部分邻居保存在同一 shard，提高查询 recall，同时保持 shard 尺寸可控且支持并行化**。

#### 4.3 **Routing策略**

一、Routing 的核心目标是什么？

Routing 的任务是：
 **给一个 query q，挑出很少几个 shard，但这些 shard 里要包含 q 的大部分 kNN。**

------

二、为什么传统的“一个中心代表一个 shard”在这里完全失效？

论文引用了经典 IVF 思想（IVF = inverted file index），其特点：

-   IVF 通常把 10⁹ 点切成 10⁶ 个小 cluster
-   每个 cluster 大小 ~1000 点
-   这种小 cluster 形状简单，一个中心即可代表

但是本论文的场景完全不同：

-   shard 数量非常小 **s ∈ [10, 100]**
-   但每个 shard 非常巨大 **|Si| > 10⁷**
-   shard 不是凸区域，不是单峰结构，是由图分割得到，形状奇怪（不规则、断裂、洞、多支路）

这就导致一个重大的几何问题：

>   **一个中心无法代表一个巨大的、非凸、内部有数十个 sub-clusters 的 shard。**

论文给出的实际数据点：

-   用单中心 routing，MS Turing 数据集的 top-1 shard recall 只有 **28%**

这说明中心路由完全不能工作。

------

三、为什么 shard “不规则、不凸” 会导致 routing 失败？

这是一个很关键但容易忽略的点。

k-means 得到的是 convex clusters（凸区域）。
 但 graph partitioning 得到的是基于图连通结构的 partition：

-   shard 可能被图结构拉成“长条”“许多分支”“多个中心”
-   shard 可能由若干个远离的 dense region 组成（sub-clusters）

所以如果你用每个 shard 的“一个中心”进行 routing：

-   query 可能离某个 sub-cluster 非常近，但离整体中心很远
     → routing 会错误地跳过本该访问的 shard
     → recall 下降

![image-20251118153609060](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251118153609060.png)

------

四、他们的核心思想：==**每个 shard 不用一个中心，而是用多点表示 Ri**==

论文提出：

**每个 shard Si ⊂ P，用一组点 Ri 来代表它的几何分布。Ri 内含多个代表点，可覆盖多个 sub-cluster。**

所有 shard 的代表点集合：

∪Ri = R（总共 m 个点）

Routing 时：

1.  给 query q
2.  在 R 中找 q 的最近邻若干点，得到 Q
3.  查看 Q ∈ 哪些 shard
4.  将 shard 按“Q ∩ Ri 中离 q 最近的距离”排序
5.  按顺序访问 shard

这叫做 **multi-center probabilistic routing**。

优势：

-   能捕捉 shard 内多个 cluster 的结构
-   即使 shard 的几何边界复杂，仍能正确判断 query 靠近哪个 shard

#####  4.3.1 **K-Means Tree Routing Index 训练与查询**

==建图：==

一、KRT 的总体思想

**目标：为每个 shard 构建一个层次化 k-means 树，用于 coarse routing。**

每个 shard Si 会独立建一棵 tree：

-   树的所有节点是 k-means 聚类中心
-   所有层的中心点共同组成代表点集合 Ri
-   查询时通过这棵树找到与 query q 最接近的若干 representative points Q
-   进而决定优先访问哪些 shard

注意：**这个树不是用来存实际点的，只存“代表点（centroids）”用于 routing**
 ——这是 KRT 与传统 k-means tree 的最大区别。

            root
       /  / ...  \  \
     c1 c2 ...  c31 c32
     /|\
    ...
>   训练过程：

![image-20251118162150032](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251118162150032.png)

P：当前递归处理的点集（最开始是某个 shard Si）。

每个节点 v 都有自己的点集 P：

-   根节点的 P 是整个 shard（Si）
-   第二层每个节点的 P 是对应的一个簇 Pc
-   第三层的 P 是 Pc 再次细分后的子簇
-   …
-   一直到 P 的大小小于 λ，就不再继续建树

s：shard 数量。

l：每个非叶节点要产生的 centroid 数。

m：全局代表点（centroid）预算，上限（routing index 总大小）。

λ：停止递归的最小簇大小阈值。

vi：第 i 个 shard 的 root 节点（树根）。

Build(P,l,m,λ,v)：对点集 P 以预算 m 构造以 v 为根的子树并把 centroids 存入 v。

按照步骤解释：

1.  并行为每个 shard 创建根并分配初始预算

![image-20251118175855851](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251118175855851.png)

-   含义：对每个 shard 单独建一棵“k-means 树”。训练是按 shard 并行的（多线程/多机友好）。
-   为什么用 `|Si| * (m-s)/|P|`：把全局预算 m 按点数比例分配给各 shard。减去 s 是因为根层（或每个 shard 至少会有一个代表）需要预留预算的一部分（实现上是为了避免把 m 全部分配给子树而忽略 root centroids）。

它决定子节点的“目标树大小”（即还能继续往下分多少层）。

2.   Build 函数 — 递归终止条件

```
if m ≤ 1 return
```

-   含义：如果给当前子树分配的预算不足以再放一个有效 centroid（或不值得继续），就停止递归，不再在此子树创建新节点。

3.   在当前节点做一次 l-means

```
centroids(v) ← K-Means(P, l)
```

-   含义：对当前子树的点集 P 做一次 k-means，生成 l 个中心向量并把它们存为当前节点 v 的 centroids。

4.   为每个 centroid 创建子簇与子节点（并行）

![image-20251118180415394](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251118180415394.png)

-   含义：把当前簇 P 按刚才的 centroids 分成 l 个子簇 Pc；为每个 Pc 创建子树根 vc（即树的下一层节点）。并行化能大幅加速构造。
-   注意：这里并没有把原始点存放到叶子（与传统 k-means tree 不同）；我们的树仅用于生成代表 centroids。

5.   子树是否继续细化：分配子树预算并递归

![image-20251118180651099](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251118180651099.png)

-   解释预算公式 `(m - l) * |P_c| / |P|`：
    -   当前节点消耗了 l 个预算（centroids），所以剩余预算是 `m - l`。
    -   这剩余预算按子簇大小比例分配给每个子簇：大子簇拿到的预算多，能继续更细化；小子簇拿到的预算少，可能很快终止。
-   `|P_c| > λ`：当子簇大小小于等于 λ 时停止对该子簇继续递归（不再细分）。

==查询：==

![image-20251118181209244](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251118181209244.png)

核心目标是：给定一个查询点 q，快速找到最可能包含其近邻的 shards（子图）。下面我帮你详细拆解。

------

1. 数据结构和初始化

```
PQ ← {(vi, 0, i) | i ∈ [s]}
min-dist[i] ← ∞ ∀i ∈ [s]
```

-   **PQ**：最小堆（min-heap），存储 `(tree node v, key, shard ID)`
    -   key = 距离 query q 的距离，用于排序
    -   heap 优先访问距离最小的节点（beam-search 样式）
-   **min-dist[i]**：记录每个 shard i 到查询点 q 的最小距离，用于最后对 shards 排序
-   初始化：把每个 shard 的根节点 vi 放入堆中，初始距离 0

------

2. 主循环（类似 beam-search）

![image-20251118184605359](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251118184605359.png)

-   每次从堆中弹出距离最小的节点 v
-   sv 表示 v 所在的 shard ID
-   b 是搜索预算（限制总迭代次数或弹出节点数），防止搜索过慢

------

3. 处理当前节点

![image-20251118184614940](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251118184614940.png)

-   遍历当前节点 v 的所有 centroids（树节点代表的簇中心）
-   更新当前 shard 的最小距离：`min-dist[sv]`
-   如果这个 centroid 有子树（非叶节点），把子节点加入堆继续搜索，key 为距离 q 的距离

**核心思想**：只对最可能包含近邻的子树继续搜索，避免全量遍历。

------

4. 终止条件

-   PQ 为空 → 搜索完所有可能节点
-   搜索预算 b 用完 → 防止搜索开销过大

------

5. 输出

```
return sort [s] by min-dist
```

-   对所有 shards 按 min-dist 排序
-   返回最可能包含近邻的 shards 列表（probe order）

##### 4.3.2 HRT Routing

 **HRT（Hash Routing）**，一种基于 **Locality Sensitive Hashing (LSH)** 的路由索引方法，用于快速确定查询点 q 应该访问哪些 shards。下面我给你详细拆解。

------

1. LSH 的基本概念

-   LSH 是一类哈希函数族 $H$，用于高维近似最近邻搜索
-   特性：
    -   相似的点 $x, y$ → 很大概率落到同一个桶：$Pr[h(x) = h(y)]$ 高
    -   不相似的点 $x, y$ → 很小概率落到同一个桶

LSH 的作用是 **把高维点映射到哈希空间，让近邻点容易聚在一起**。

------

2. 构建 SortingLSH Index

1.  **采样点**

    -   从数据集 P 中均匀随机采样 m 个点 $R \subset P$

2.  **多次哈希**

    -   对每个点 x ∈ R，使用 t 个独立 LSH 函数 $h_1, ..., h_t$
    -   得到复合哈希串：$(h_1(x),...,h_t(x))$

3.  **排序**

    -   按照复合哈希串进行字典序排序

    每个点的复合哈希串是：
    $$
    (h_1(x), h_2(x), ..., h_t(x))
    $$
    其中每个 $h_i(x) \in \{0,1\}$，所以整个串就是一串 0/1 组成的长度 t 的位串。

    排序规则：

    **按从左到右逐位比较，遇到第一个不同的比大小（0 < 1），完全一样则平级。**

    也就是：

    -   把复合哈希串当作二进制字符串
    -   或当作长度 t 的数组
    -   按字典序排序即可

    例子：

    | 点   | 复合哈希串 | 排序顺序 |
    | ---- | ---------- | -------- |
    | A    | 0 1 0 0    | 第 1     |
    | B    | 0 1 1 0    | 第 2     |
    | C    | 1 0 0 1    | 第 3     |

    -   相似点更可能有更长的前缀 → 排序后靠得更近

4.  **重复构建**

    -   为提高召回率，重复 r 次（例如 r=24），得到多个排序索引

    **每重复一次，就创建一个新的、完全独立的 LSH 索引：**

    对每次重复 i：

    1.  用新的随机 hash 函数集合
         （即新的 h1, h2, ..., ht）
    2.  对 m 个代表点 R 计算新的复合 hash 串
    3.  lexicographically 排序
    4.  存成 Qᵢ（第 i 个 Sorted-LSH index）

    最终得到：

    ```
    Index_1: sorted by hash set #1
    Index_2: sorted by hash set #2
    ...
    Index_r: sorted by hash set #r
    ```

    论文里 r ≈ 24
    也就是说你有**24 个不同的排序表**

    **查询时怎么用这些重复构造？**

    查询一个向量 q 时：

    对每个 index_i：

    1.  用该 index 的 hash 函数计算 h(q)
    2.  找到 h(q) 在排序表里最接近的位置 τ
    3.  取窗口 [τ-W, τ+W] 作为候选

    最后将所有 index 的候选合并，得到 Q。

**直观理解**：
 排序后的列表类似“一维近邻序列”，查询点只需要查附近窗口，就能快速找到潜在近邻点所在的 shard。

------

3. Routing 查询流程

1.  给定查询点 q
2.  对每个 r 次索引：
    -   使用同样的 t 个 LSH 函数得到复合哈希串 $h(q)$
    -   在排序列表中找到 **最接近 $h(q)$ 的位置 τ**
    -   取窗口 [τ − W, τ + W] 中的点，计算实际距离，确定对应的 shards

**核心思想**：用 LSH 将 query 映射到排序位置，再在局部窗口搜索。

------

4. 理论保证

-   HRT 的优点之一是可以提供 **形式化的理论保证**
-   虽然路由只保证 **至少有一个近似最近邻所在的 shard 被访问**
-   但在自然分区假设下，大多数近邻会和最近的点落在同一个 shard → 最终可以找到真实最近邻

------

5. 总结对比 KRT

| 特性     | KRT (k-means Tree Routing)   | HRT (Hash Routing)                   |
| -------- | ---------------------------- | ------------------------------------ |
| 构建方式 | 分层 k-means 树              | 采样 + LSH 多次哈希 + 排序           |
| 数据结构 | 树                           | 多个排序数组（排序 LSH）             |
| 查询方式 | beam-search 树遍历           | 哈希映射 + 排序窗口查找              |
| 理论保证 | 无明确理论保证（实验效果好） | 可证明至少路由到包含近似 NN 的 shard |
| 优势     | 精度高，适合大 shard         | 构建快速，可提供理论保证             |



# Prefetching

### 1. Graph Reordering for Cache-Efficient Near Neighbor Search

==摘要：==

图搜索在近邻搜索中非常高效，但遍历图时容易出现缓存未命中，导致速度下降。作者通过图重排，将经常一起访问的节点在内存中靠近存放，从而优化内存布局，提高缓存命中率。实验表明，这种重排能将查询速度提升最多 40%，而重排本身耗时相比构建索引几乎可以忽略。

==图搜索索引组成：==

这部分主要在讲 **图搜索类 ANN 索引（HNSW、PANNG、ONNG 等）内部都有哪些核心组件，以及它们为什么可以一并讨论**。

**核心思想：这些图索引虽然名字不同，但结构和搜索流程极其相似，所以图重排的效果可以推广到它们全部。**

------

**图搜索索引的三个核心组成**
 • **构图阶段的 pruning / diversification（剪枝和多样化）**：把不必要的边删掉，把一些有用的长边补上，让图更“好走”。
 • **搜索初始化**：给 beam search 找一个起点。可以是 HNSW 的上层结构、LSH、树结构或随机点。
 • **Beam search 本身**：沿着图走，维护一个大小 M 的候选池，不断扩展更近的点。

作者强调：尽管不同算法细节不同，但访问节点的顺序和图结构很相似，所以图重排的适用性是通用的。

------

**k-NN 图和 ε-NN 图的区别**
 • k-NN：每个节点连向距离最近的 k 个点（有向图）。
 • ε-NN：距离小于 ε 的节点全部连接（无向图）。
 两者都可能产生不连通区域。

------

**为什么真实近邻图不能直接构？因为太贵（O(N²)）**
 所以现代方法都用“bootstrapping”——边建边查：

1.  新点 q 查询目前的图（用近似搜索），得到近邻；
2.  更新这些邻居的边；
3.  最终成形的是 **近似 k-NN** 或 **近似 ε-NN** 图。

NN-Descent 是另一种常用方法（反复更新，几轮后接近真实图）。

------

**现代最强的 ANN（如 HNSW 和 PANNG/ONNG）都不是用原始 k-NN 图，而是构图后进行剪枝与边补充**：
 • **HNSW**：分层结构，高层点更少，用于快速定位区域。底层是 pruned k-NN。其剪枝规则来自 Arya & Mount：如果邻居 B 比不上通过另一个邻居到达的路径，则删边。
 • **PANNG / ONNG**：思想类似，基本都是“如果有更短路径就删掉长边”。ONNG 还会保证每个点的入度≥kI。
 • 其他如 FANNG，本质 prune 规则也差不多。

作者指出：虽然方法多，但因为剪枝原则接近，所以这些图的结构非常类似，这正是为什么重排效果可泛化。

------

**搜索过程：本质是沿图步行**
 对每个访问的节点 x，计算 d(q, x)，然后从它的邻居中继续走，看能否找到更近的点。直到 M 个候选都走不动，就返回最好的几个。

**Beam search 是主要算法；greedy search 是它的 M=1 特例。**

------

**初始化在搜索中占比很小，而且不同初始化方式差别不明显**
 作者用 SIFT1M + HNSW 实验显示：
 • 层级初始化只占 **约 3%** 的时间
 • 把层级换成“随机选 50 个点取最近的”完全不影响效果

结论：初始化方法对性能影响极小，因此不会影响图重排实验的有效性。

#### 1.1 图重排实现：

**把图的节点重新编号，使得在内存中相邻的节点，更可能在搜索时被连续访问，从而降低 cache miss。**

------

**一、图重排的目标是什么？**

把每个节点 v 映射到一个新的编号 P(v)，并按照 P(v) 的顺序放入数组 = 内存布局。
 目标是：
 **如果两个节点经常一起访问，就把它们的编号排得越接近越好。**

这样 CPU 访问内存时，一旦把某段加载进 cache，就能命中更多后续访问。

------

**二、三类重排方法：优化式、基于局部统计、基于图划分**

论文把现有方法分成三大类。

------

**（1）优化式方法：直接优化“标签接近 ⇔ 关系强”。**
 这一类是论文最看重的，也是实际效果最好的。

典型代表：

1.  **Gorder**
     目标：让连续的 w 个节点之间邻居尽量重叠。
     含义：把强相关节点放一起。
     优点：强，但计算成本高（NP-hard 的近似）。
2.  **RCM**
     目标：减少“两个相连节点的编号差”。
     含义：边尽量靠近对角线，让图变“带宽小”。
     优势：老牌算法，本质是 BFS 排序。
3.  **MLOGA / MLINA**
     目标：最小化所有边的编号差距（总和或 log)。
     含义：尽量让所有边都“短”，而不是只管最大差距。

共同特点：
 **都是 NP-hard，只能用启发式，但能明显改善 cache 行为。**

------

**（2）基于节点局部特征的方法：如 Degree Sorting**
 做法：按节点度从大到小排序。

直觉：
 高度节点常常连接彼此（尤其在 power-law 图）。
 所以排在前面的高程度节点很可能会一起被访问。

但对于 ANNS 图（如 k-NN 图）：

• 度数都差不多（每个节点 k 条边）
 • 不呈 power-law 分布

→ 这种方法在 ANNS 上基本无效。

------

**（3）基于图划分的方法：把每个 partition 放一段连续内存**

思想：
 图划分算法会把 tightly-connected 的节点放到同一区域。
 我们可以给每个 partition 一个连续的 label 区间。

缺点：
 分区结果可能不适合 ANNS 的访问模式。

------

**三、为什么优化式方法明显更好？**

因为 ANNS 图的访问模式是：

• 查询时沿着图进行 local walk（beam search）
 • 访问的是“局部邻域 + 一些分层入口节点”
 • 每次跳转都依赖邻居布局

因此最关键的是：

**把邻接的节点尽可能放在相邻的内存地址。**

优化式方法正是直接围绕这个目标设计的，而 degree-sorting 之类的根本不关注“邻居访问顺序”。

这就是为什么论文说：

>   轻量算法在 ANNS 里没用，objective-based 才有效。

------

**四、为什么重排不会让构建成本爆炸？**

尽管这些 objective-based 算法看起来贵，但：

• 重排图的成本通常是 O(E log N) 或 O(E) 级别
 • 构建图（如 HNSW）成本本来就非常高
 • 因此重排耗时反而比构建图小一个数量级

论文说：

**重排 ≪ 建图成本
 而加速可达 10–40%**

工业上很划算。

------

**五、为什么 ANNS 图与社交网络不同？**

大部分重排算法最初用于 social graph，这些图有：

• power-law degree（少数超级节点）
 • 稀疏结构
 • 社区结构明显

但 ANNS 图（如 k-NN）：

• 度均匀，不是 power-law
 • 较密
 • 空间结构驱动，而不是社交结构

因此：

**很多社交图能用的重排技巧，在 ANNS 上不一定有效。**

==**实验设计**==

-   使用 HNSW 索引，对大规模 embedding 数据集（SIFT100M/1B、DEEP100M/1B）进行实验。
-   指标包括 **平均查询时间、P99延迟、缓存未命中率、top-100 recall**。
-   控制单核运行、warm start、固定查询集合等外部因素。

**实验结果**

![image-20251119190819252](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251119190819252.png)

-   Objective-based 重排（Gorder/RCM）可显著提高查询速度：**小数据集 10% 提升，大数据集高 recall 下可达 40%**。
-   重排时间远小于索引构建时间，长期查询收益远超一次性成本。
-   缓存命中率显著提高，降低 L1/L2/L3/TLB miss。



### 2. Graph Prefetching Using Data Structure Knowledge

**1. 背景与问题**

-   内存受限的工作负载（memory-bound workloads）通常通过 **预取（prefetching）** 提升吞吐量：硬件根据访问模式预测未来内存访问，将数据提前加载到高速缓存。
-   对于图计算，传统硬件预取器效果很差，因为图访问模式是 **数据依赖（data-dependent）** 且非连续的：
    -   **Stride prefetcher** 无法处理非规律的访问序列。
    -   **Correlation-based prefetcher** 存储开销大，且对一次性计算提升有限。
    -   **Pointer prefetcher** 无法有效预测间接数组访问。
-   类似 MapReduce 的框架对图计算也不高效，因为图计算通常 **不可天然并行**，迭代次数多，开销大。

**2. 论文的核心观察**

-   尽管硬件无法自动捕捉图结构，但在常见图应用中，**访问模式其实是可预测的**，例如 BFS、DFS、Best-First Search 等遍历模式。
-   因此，可以使用 **显式预取器（explicit prefetcher）**：硬件预取器利用对数据结构和遍历方式的知识，提前知道需要哪些数据，减少学习访问模式的开销，并能提前做出预取决策。

**3. 方法**

-   论文针对 **压缩稀疏行（Compressed Sparse Row, CSR）格式图**，设计了一个硬件预取器，专门优化 BFS 遍历。
-   BFS 是社交网络分析、网页抓取、模型检测等领域常用的计算核，很多算法都基于 BFS。
-   CSR 是稀疏图的标准存储格式，存储高效、被广泛采用；相比其他表示方法（如 GraphLab），CSR 减少了额外开销（如数据复制）。
-   预取器通过 **监控内存队列中的读取操作**（snoops reads），计算并调度边和顶点数据提前加载到 L1 缓存，从而保证数据在访问时已在高速缓存中。

==相关方法介绍：==

**第一段：Compressed Sparse Row（CSR）数据结构**

这段说明了 **CSR 是一种非常高效的稀疏矩阵/图存储格式**，常用于图处理，因为它既节省空间又性能好。

CSR 由两个核心数组组成：

![image-20251120163608083](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251120163608083.png)

1.  **edge list（边数组）**
    -   一维数组，顺序存储图中所有顶点的所有邻居。（从0开始，其邻居为1，2；然后到1，其邻居为7；然后到2，其邻居为0，6）
2.  **vertex list（顶点数组）**
    -   记录每个顶点的邻居在 edge list 中的起始位置。（从0开始，其邻居在edge list中从0开始；然后到1，其邻居从2开始；然后到3，其邻居从3开始）

CSR 图结构不包含指针，只有索引，因此：

-   更紧凑
-   更适合 CPU 缓存和内存顺序访问

论文图 1(a) 是一个示例图，而图 1(b) 则展示它的 CSR 两个数组。

------

**第二段：关于 BFS 的背景**

BFS（Breadth-First Search，广度优先搜索）是图计算中最常用、最重要的访问模式之一，广泛用于：

论文指出：
 **由于 BFS 如此常见，优化 BFS 的内存访问对整个图计算性能具有巨大意义。**

------

**第三段：BFS 的执行过程（Algorithm 1与 Figure 1(b)）**

论文描述 BFS 工作方式：

![image-20251120164133903](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251120164133903.png)

1.  从起始节点开始。
2.  将其加入一个 FIFO 队列（work list queue）。
3.  每次从队列取一个顶点，通过 CSR：
    -   用 vertex list 找到它的邻居起点
    -   用 edge list 遍历它的所有邻居
4.  每访问到一个新的邻居，就将它加入队列。

论文给了一个例子：
 从顶点 5 开始遍历，访问顺序为：**（work-list）**

**![image-20251120164303915](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251120164303915.png)
 5 → 4 → 1 → 2 → 3 → 7 → 0 → 6**（按照顺序来，5->4；然后遍历4的邻居，1，2，3；然后是1的邻居7；然后是2的邻居0，6）

这符合 BFS 的“逐层”访问方式。

==BFS的缺陷：==

**Stalling Behavior（停顿行为分析）** 的逐句清晰解释，让你更容易理解论文为什么强调 BFS 的访存瓶颈。

------

**1. BFS 的核心问题：访存局部性极差**

论文开头强调：

-   **vertex list（顶点数组）访问没有时间局部性或空间局部性**
     → 每次访问不同的顶点，跳得很随机，连续性差。
-   **edge list（边数组）只有“单个顶点内部”有局部性**
     → 访问一个顶点时会顺序读它的所有邻居，但访问下一个顶点就跳走了。

因此：

**一旦图比最后一级缓存（LLC）大，缓存几乎帮不上忙 → 性能由内存延迟主导。**

------

**2. 实测：BFS 会造成极高的 stall（停顿）**

论文引用 Graph500（图搜索标准 benchmark），在 Intel Core i5-4570 上实验：

-   **stall rate 接近 90%**
     → CPU 有 90% 的时间都在等待数据从内存返回（停顿），不是在计算。
-   **随着图规模变大，stall 率还在继续上升。**

这是图计算性能低下的核心原因。

------

**3. 为什么 stall rate 这么高？因为 L1 cache miss 非常高**

图 2(b) 显示：

![image-20251120164633747](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251120164633747.png)

-   L1 cache miss rate 接近 **50%**

对 CPU 来说，L1 miss 非常昂贵，会导致 pipeline 停顿，一路等到 L2/L3 或内存返回。

BFS 的访问模式导致 cache 几乎无法命中，因为访问是随机散布的。

------

**4. 更精细的数据（图 2(c)，gem5 仿真结果）**

他们用 gem5 模拟器深入分析 miss 延迟来源（scale 16, edge factor 10）：

-   **69% 的额外时间来自 edge list misses**
     → edge list 非常大（edge list ≈ vertex list 的 20 倍），且访问跳跃大。
-   **25% 来自 “visited” 数组 misses**
     → visited[v] 标记顶点是否访问过
     → 类似 vertex list，访问顺序随机，每次访问几乎都是 L1 miss。

尽管 visited 的大小与 vertex list 相同，但：

-   每条边访问一次 visited
-   次数非常多（几千万次）
-   且顺序完全随机
     → 导致大量 cache miss

这为后面提出的“硬件预取器”提供了直接动机：

-   如果能提前把 edge list / vertex list / visited 数据拉进 L1 cache
     → 就能显著减少 stall
     → 提升 BFS 性能（论文中加速 2.3×–3.3×）

==传统预取技术的局限性：==

**第一部分：Stride Prefetching（步幅预取）为什么失效**

现代 CPU 最常用的预取器是 **stride-based**（按固定步长推测未来访问）。

它的假设：
 下一次访问地址 ≈ 上一次地址 + 固定步长

这种模式适用于：

-   顺序扫描数组
-   遍历矩阵
-   连续内存结构

但 BFS 访问模式是：

-   每个顶点的邻居列表虽然连续
-   **顶点之间跳跃极大且不可预测（数据依赖）**

因此：

**没有可学习的 stride，预取器形同虚设。**

------

**第二部分：为什么不能预取 visited list（访问标记数组）？**

作者指出：

-   visited[v] 属于随机访问
-   想预取 visited list，就必须先知道 v 的值
     然而 **v 的值本身来自 edge list 的读取**

意味着：

**预取 visited 需要等待 edge list 加载 → 预取本身要 stall → 失去意义**

图中示意：

![image-20251120165149751](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251120165149751.png)

![image-20251120165720152](C:\Users\pydu\AppData\Roaming\Typora\typora-user-images\image-20251120165720152.png)

-   visited list 分布在多个 cache line（**内存不是按字节读的，而是按块读的**：当 CPU 访问某个地址时，内存控制器会把“该地址所在的 64 字节块”整块加载进缓存，这个块就是 **cache line**）

**visited list 是什么？**

通常是一个布尔数组：

```
visited[0], visited[1], visited[2], ..., visited[N-1]
```

N = 顶点数（可能是百万、千万、甚至上亿级）

即使每个 visited 只占 1 byte 或 1 bit，总大小也很容易超过：

-   L1 cache（32 KB）
-   L2 cache（256 KB）
-   甚至 L3 cache（几 MB）

------

**原因 1：数组太大，必然跨多个 cache line**

假设：

-   cache line = 64 byte
-   visited 数组 = 10M 个顶点（常见规模）

即使每个 visited 用 1 byte 存：

-   总大小 = 10 MB
-   需要数量 = 10MB / 64B = **约 156,000 个 cache line**

所以显然会跨很多 cache line。

-   BFS 会随机访问其中的单个标记
-   大量随机 prefetch 会把 CPU 流水线压爆，反而变慢

------

**第三部分：Software Prefetching（软件预取）也不好用**

软件预取 = 程序员在代码里手动插入 prefetch 指令。

理论上 BFS 的访问模式已知，程序员可以：

-   预取 vertex list
-   预取 edge list
-   预取 visited list

但论文分析了实际问题：

------

**问题 1：软件预取不能依赖前一个预取的结果**

例如：

```
prefetch(vertex_list[work[i]])
```

要执行 prefetch，你要先读取 work[i] → 这就会 stall。

因此：

**任何软件预取都会引入额外 load → 造成 stall → 抵消预取的收益**

------

**问题 2：预取距离（prefetch distance）是动态变化的**

-   BFS 队列的长度不断变化
-   不同图的度分布也不同

但软件预取必须设置 **固定距离**：

-   距离小 → 太晚，来不及
-   距离大 → 太早，会被 eviction 掉

软件无法动态适应 workload 行为。

------

**问题 3：预取 edge list 量巨大，会产生大量指令**

每个顶点有很多邻居：

-   若你对每个 neighbor 都 prefetch → 指令爆炸
-   指令开销 > 预取收益

所以不可行。

------

**论文结论：最佳的软件预取策略仍然很有限**

作者发现：

-   最好的策略是：
     在 BFS 算法中的第 4 与第 5 行之间
     对 **未来某个顶点的前两条 edge-list cache line** 做 prefetch

但是：

-   调不同 offset、不同 cacheline 数目都无提升
-   尝试预取 vertex list、work list 都让性能下降

最终结果（Figure 3(b)）：

-   软件预取 + 硬件预取可提升约 **35% 性能**
-   但 CPU 仍然有 **80% 的时间在 stall**

说明：

**传统预取技术远远不够，依旧留下大量性能空间。**